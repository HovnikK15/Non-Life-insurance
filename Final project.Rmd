---
title: "Final project"
output: html_document
author: "Klemen Hovnik and Manca Strgar"
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages,  include=FALSE, warning = FALSE, message = FALSE}
#package instalation:
packages <- c("tidyverse", "here", "gridExtra", "grid", "rstudioapi", "MASS", "actuar", "statmod", "ReIns", "pscl")
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)

if(sum(!(packages %in% installed.packages()[, "Package"]))) {
  stop(paste('The following required packages are not installed:\n', 
             paste(packages[which(!(packages %in% installed.packages()[, "Package"]))], collapse = ', ')));
} else {
  message("Everything is set up correctly. You are ready to go.")
}
#libraries:
library(dplyr)
library(knitr)
library(MASS)
library(rmarkdown)
library(tidyverse)
library(ggplot2)
library(pscl) 
```
# 1 Importing dataset

```{r, echo=F}
#Okolje, kjer pišeš kodo - Ta celotni kvadratek skopiras, ko kaj rabiš
```

First, we need to import Insurance contract data from the file `NonFleetCo507` into R. The file contains 159.947 contracts with 11 variables. The column `Clm Count` shows how many claims were filed on the contract. This is the loss frequency data that we will use for model fitting. The column `TLength` shows the period of exposure during which the contract was active and the Clm Count was filed. 

```{r, echo=F, class.output="bg-warning"}
#KK SE TI ZDI ČE JE OUTPUT OKVIRČEK TAKE RUMENE BARVE NAMEST BELE? 
mtpl_orig <- read.delim("NonFleetCo507.txt", header = TRUE)
mtpl_orig=as_tibble(mtpl_orig)
str(mtpl_orig)
#summary(mtpl_orig)
kable(head(mtpl_orig))

```
**Tu lahko dodama opis tabele, torej kaj kaj predstavlja, kaj je max, min..

We could rename colums in our dataset to make it easier to work with. We will rename the column `Clm_Count` into `nclaims` as the number of claims, and `TLength` into `expo` as exposure. For easier programming we will also rename `AgeInsued` and `SexInsured` into `age` and `sex`.
```{r, echo=F}
#We don't need all the colums, so we can make new table which has only imporant colums about #insurance contract
mtpl <- mtpl_orig%>% dplyr::select(-PrivateCar, -VehCapCubic, -VehCapTonn)
#dim(mtpl)
mtpl <- mtpl %>%
  # rename all columns 
  rename_all(function(.name) {
    .name %>% 
      # replace all names with the lowercase versions
      tolower 
    })
mtpl <- rename(mtpl, nclaims = clm_count,expo = tlength, sex = sexinsured, age = ageinsured )

```

# 2 Empirical analysis (1. vprašanje)

To have a better understanding of our data we can start by plotting number of claims per insurance contract on the graph. We can observe that the number of actual claims represents only a small proportion of all insurance contracts.

```{r, echo=F}
k<- ggplot(mtpl, aes(nclaims)) + geom_bar(col = "red", 
              fill = "red", alpha = 0.5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims per contract")
k
```

Looking at this in relative terms, we can see that more than 90% of insurance contract did not have any claims. 

```{r, echo=F}
g <- ggplot(mtpl, aes(nclaims)) + theme_bw()
g + geom_bar(aes(y = (..count..)/sum(..count..)), 
    col = "red", fill = "red", alpha = 0.5) + 
  labs(y = "Relative frequency") +
  ggtitle("MTPL - relative number of claims")
```



Continuing our analysis, we can compute the mean and variance of the number of observed claims. 

```{r, echo = F, warning=FALSE}
mean1 <-  sum(mtpl$nclaims) /length(mtpl$nclaims)
var1 <-  sum((mtpl$nclaims - mean1)^2) /length(mtpl$nclaims)
c(mean = mean1, variance = var1)

```

Because we have different exposures between policyholders, we could take exposure information `expo` into account as well. Then we can calculate empirical claim frequency, per unit of exposure and variance. 

```{r first-risk-calculations-mtpl-2, echo = F, warning=FALSE}
mean2 <- sum(mtpl$nclaims) / sum(mtpl$expo); 
var2 <- sum((mtpl$nclaims-mean2*mtpl$expo)^2)/sum(mtpl$expo) 
mean2
c(mean = mean2,   variance = var2)
#mtpl %>% summarize(emp_freq = mean2, variance = var2 )
```

If we do the same for each gender we see that claim frequency is higher for males than females.

```{r, echo = F, warning=FALSE, message=FALSE}
mtpl %>%  group_by(sex) %>% summarize(emp_freq = sum(nclaims) / sum(expo))
```

# 3 Fitting different distributions to the loss frequency data

We will fit different distribution to the observed claim count data. We will take exposure into account as well and will fit the distribution using Maximum Likelihood Estimation (MLE)

## 3.1 Poisson distribution (2. vprašanje)

### 3.1.1 Numerical calculation

We start by fitting the Poisson distribution to the observed loss frequency data.


Since, not all policyholders are insured throughout the whole year (exposure = period of exposure during which the contract was active, is not equal to 1 for all the contracts), we assume that the claim intensity is proportional to the exposure. For Poisson distribution this means that the intensity is equal to $\lambda * expo$. For `expo=1` (contract active for a whole year), the expected number of claims equals to $\lambda.$ For all other contracts, the expected number of claims equals  $\lambda * expo$.
$N_i \sim Poiss(\lambda * expo)$

Let denote that $m$ is the number of observations and $n_{i}$ is the observed number of claims for i-th policyholder, then general definition of likelihood is given by:
$$L(\lambda)= \prod_{i = 1}^{m} P(N_i = n_i) =\prod_{i = 1}^{m} exp(-\lambda \cdot expo) \cdot \frac{(\lambda \cdot expo)^{n_i}}{n_i!} $$

$$l(\lambda)=\sum_{i = 1}^{m} -\lambda \cdot expo + n_i \cdot log(\lambda \cdot expo) - log(n_i!)$$


We can maximize loglikelihood with respect to $\lambda$. 
```{r, echo=F}
expo = mtpl$expo
nclaims = mtpl$nclaims
```

```{r, echo = F}
poisson.loglikelihood <- function(lambda)
{
  loglikelihood <- sum(-lambda * expo + nclaims * log(lambda * expo) - lfactorial(nclaims))
  
  return(loglikelihood)
}

```
In practice, we minimize the negative log-likelihood so that we can use `nlm` (non-linear minimizer) function for finding the minimum.Minimizing the negative log-likelihood is equal to maximizing the log-likelihood.

$$l(\lambda)=\sum_{i = 1}^{m} \lambda \cdot expo - n_i \cdot log(\lambda \cdot expo) + n_i!$$

Since the parameter $\lambda$ is strickly positive, we will reparametrize the likelihood and optimize for $\beta = log(\lambda)$ which can take values $(-\infty, \infty)$

```{r, echo = F}

poisson.negLoglikelihood <- function(beta)
{
  lambda = exp(beta)
  
  return(-poisson.loglikelihood(lambda))
}
```

Now we will use non-linear minimization function to get the minimum value. The starting value will be set to $\beta = 1$. The function returns the following lines:

- minimum: the value of estimated minimum of our functionž

- estimate: the point at which that minimum is obtained.

- gradient: first derivative at the estimated minimum 

- hessian: second derivative at the estimated minimum

```{r, echo = F, warning=FALSE}

fit <- nlm(poisson.negLoglikelihood, 1, hessian = TRUE)
fit
```

From this we can get the estimation of $\lambda$ which is identical to the expected value (empirical claim frequency per unit) that we have calculated before.

```{r, echo = F}
poisson.lambda <- exp(fit$estimate) 
poisson.lambda  #estimated value of lambda
```

In general, the covariance matrix of the maximum likelihood estimators can be estimated by the inverse of this hessian of minus the log-likelihood at the estimated minimum. By taking the square root of these diagonal elements, we obtain the corresponding standard error that equals:
```{r, echo = F}
sqrt(diag(solve(fit$hessian)))
```

### 3.1.2 Akaike Information Criterion

Calculation of Akaike Information Criterion (AIC):
Let $k$ be the number of estimated parameters in the model and let $L$ be the maximum value of the likelihood function for the model. Then the AIC value of the model is:
$$AIC = 2k - 2ln(L)$$
```{r, echo = F}
poisson.minimum <- poisson.loglikelihood(poisson.lambda)
AIC_poi = -2 * poisson.minimum + 2 * 1
AIC_poi
```

### 3.1.3 Calculation with the help of Generalized Linear Models (GLMs)

We will now vertify our solution in the previous section for numerically computed log-likelihood with the `glm` function. We will put focus on the Poisson regression model. Under Poisson assumption in this model, the number of claims is distributed as follows:
$$N \sim Poiss(\mu),$$ $$\mu = expo \cdot exp(x^`\beta).$$

```{r, echo = F, warning=FALSE}

neg_loglik_pois <- function(par, freq, expo){
  lambda <- expo*exp(par)
  -sum(dpois(freq, lambda, log = T))
}
fit_poi <- nlm(neg_loglik_pois, 1, hessian = TRUE, 
               freq = mtpl$nclaims, expo = mtpl$expo)
fit_poi

```
We can see that we get the same values of the estimate, and minimum. 

We can also calculate correspoding standard deviation:

```{r, echo = F}
sqrt(diag(solve(fit_poi$hessian)))
```

The value of AIC also stays the same and it is equal to:
```{r, echo=F}

AIC_poi2 <- 2*length(fit_poi$estimate) + 2*fit_poi$minimum
AIC_poi2

```

We can conclude, that both methods return the same results.

## 3.2 Negative Binomial distirbution
### 3.2.1 Numerical calculation

The pobability function for the negative binomal distribution is:
$$P(N=k)=\frac{\Gamma(a+k)}{\Gamma(a)k!}(\frac{\mu}{\mu+a})^k(\frac{a}{\mu+a})^a$$
where $\mu$ is the expected number of claims for a policy holder who is insured for a full year. So for each policy holder we can define $\mu_i = expo\cdot \mu.$

The ??Log function equals:
$$L(\lambda)= \prod_{i = 1}^{m} P(N_i = n_i)$$
$$l(\lambda)=\sum_{i = 1}^{m} log(\Gamma(a+k)) - log(\Gamma(a)k!)+k\cdot log(\frac{\mu}{\mu+a})+a\cdot log(\frac{a}{\mu+a}).$$

```{r, echo=F}
#From empirical analysis we know that: 
mu <- sum(mtpl$nclaims) / sum(mtpl$expo)
var <- sum((mtpl$nclaims-mu*mtpl$expo)^2)/sum(mtpl$expo) 

NB.negativeLoglikelihood <- function(beta)
{
  mu <- exp(beta[1])
  a <- exp(beta[2])
  
  loglikelihood <- sum(lgamma(a + nclaims) - lgamma(a) - lfactorial(nclaims) + nclaims * log(mu*expo/(mu*expo + a)) + a * log(a / (mu * expo + a)))
  
  return(-loglikelihood)
}
```

We know that $\mu=E(X)$ $a=\frac{\mu^2}{Var(X)-\mu}$

```{r, echo=F}
#Starting point:
mean <- mu
a0 <- mu^2 / (var - mu) 
# we know that beta[1] = log(mu) and beta[2]=log(a0)

fit2 <- nlm(NB.negativeLoglikelihood, log(c(mean, a0)),hessian=TRUE)
fit2

```
The corresponding standard error equals:
```{r, echo = F}
sqrt(diag(solve(fit2$hessian)))
```
### 3.2.2 Akaike Information Criterion
```{r, echo=F}
nb.loglik <- -fit2$minimum
AIC_nb = - 2 * nb.loglik + 2 * 2
AIC_nb
```
### 3.2.3 Calculation with the help of Generalized Linear Models (GLMs)
```{r, echo = F, warning=FALSE}

neg_loglik_nb <- function(par, freq, expo){
  mu <- expo*exp(par[1])
  r <- exp(par[2])
  -sum(dnbinom(freq, size = r, mu = mu, log = TRUE))
}
fit_nb <- nlm(neg_loglik_nb, c(1, 1), hessian = TRUE, 
              freq = mtpl$nclaims, expo = mtpl$expo)
fit_nb
##fit_nb$estimate
#exp(fit_nb$estimate)
```

We get the same standard deviation as from the numerical calculaton:
```{r, echo=F}
sqrt(diag(solve(fit_nb$hessian))) 

```

The variance estimated by this Negativ Binomial model is:
```{r, echo = F}
exp(fit_nb$estimate[1]) + (exp(fit_nb$estimate[1])^2)/(exp(fit_nb$estimate[2]))
```


## 3.3 Zero Inflated Poisson  distirbution
### 3.3.1 Numerical calculation
This is a Poisson distribution where the probability of having zero claims is increased by $p$.

$$ P(N^{ZI}=k) =   \left\{
\begin{array}{ll}
      p+(1-p) \cdot P(N=0) ; k=0, \\
      (1-p) \cdot P(N=k) ; k>0 \\
\end{array} 
\right.  $$
where $N$ represents Poisson distribution. 
The parameter $p$ takes valued in $[0,1]$, so we can transform in to the real line $(-\infty,\infty)$ with logarithm:
$$logit(p)=log(\frac{p}{1-p})=\beta$$ 
$$p=\frac{exp(\beta)}{1+exp(\beta)}$$
```{r, echo = F}
ZIP.negativeLoglikelihood <- function(beta)
{
  lambda <- exp(beta[1])
  p <- exp(beta[2])/(1+exp(beta[2]))
  
  density <- (p + (1-p) * exp(-expo * lambda))^(nclaims == 0) * ((1-p) * exp(-expo * lambda) * (expo *lambda)^nclaims / gamma(nclaims+1))^(nclaims != 0) 
  
  loglikelihood <- sum(log(density))
  
  return(-loglikelihood)
}
```

```{r, echo=F, warning=FALSE}
fit3 <- nlm(ZIP.negativeLoglikelihood, c(0, 0),hessian=TRUE)
fit3
```



```{r, echo=F}
ZIP.lambda <- exp(fit3$estimate[1])
ZIP.p <- exp(fit3$estimate[2])/(1+exp(fit3$estimate[2]))
c(lambda = ZIP.lambda, p = ZIP.p)

```

The corresponding standard error equals:
```{r, echo = F}
sqrt(diag(solve(fit3$hessian)))
```

### 3.3.2 Akaike Information Criterion

```{r, echo=F}
ZIP.loglik <- -fit3$minimum
AIC_zip = -2 * ZIP.loglik + 2 * 2
AIC_zip
```

### 3.3.3 Calculation with the help of Generalized Linear Models (GLMs)

```{r, echo=F, warning==FALSE, message=FALSE}
neg_loglik_ZIP <- function(par, freq, expo){
  lambda <- expo*exp(par[1])
  p <- exp(par[2])/(1+exp(par[2]))
  -sum((freq == 0) * (log(p + (1-p)*dpois(0, lambda, log = FALSE)))) - 
    sum((freq != 0) * (log((1-p)) + dpois(freq, lambda, log = TRUE)))
}
fit_ZIP <- nlm(neg_loglik_ZIP, c(1, 1), hessian = TRUE, 
              freq = mtpl$nclaims, expo = mtpl$expo)
fit_ZIP
```

Corresponding standard errors:

```{r, echo=F}
fit_ZIP_se <- sqrt(diag(solve(fit_ZIP$hessian)))
fit_ZIP_se
```

## 3.4 Hurdle Poisson distirbution
### 3.4.1 Numerical calculation
In Hurlde Poisson the probability of observing zero claims is set to $p$. The probability of observing $k$ claims equals:
$$ P(N^{H}=k) =   \left\{
\begin{array}{ll}
      p ; k=0, \\
      (1-p) \cdot \frac{(N=k)}{1-P(N=0)} ; k>0 \\
\end{array} 
\right.  $$
Here the probability $p$ does not depend on the exposure, while intensity $\lambda$ is proportional to exposure as: $\lambda_i=expo\cdot \lambda.$

```{r, echo=F}
Hurdle.negativeLoglikelihood <- function(beta)
{
  lambda <- exp(beta[1])
  p <- exp(beta[2])/(1+exp(beta[2]))
  
  density <- (p)^(nclaims == 0) * ((1-p) * exp(-expo * lambda) / (1-exp(-lambda * expo)) * (expo *lambda)^nclaims / gamma(nclaims+1))^(nclaims != 0) 
  
  loglikelihood <- sum(log(density))
  
  return(-loglikelihood)
}
```


```{r, echo=F, warning=FALSE}
fit4 <- nlm(Hurdle.negativeLoglikelihood, c(0, 0),hessian=TRUE)
fit4
```
```{r, echo=F}
Hurdle.lambda <- exp(fit4$estimate[1])
Hurdle.p <- exp(fit4$estimate[2])/(1+exp(fit4$estimate[2]))
c(lambda = Hurdle.lambda, p = Hurdle.p)

```

The corresponding standard error equals:
```{r, echo = F}
sqrt(diag(solve(fit4$hessian)))
```
### 3.4.2 Akaike Information Criterion

```{r, echo=F}
Hurdle.loglik <- -fit4$minimum
AIC_Hurdle = -2 * Hurdle.loglik + 2 * 2
AIC_Hurdle
```

### 3.4.3 Calculation with the help of Generalized Linear Models (GLMs)

```{r, echo=F, warning=FALSE, message=FALSE}
#Bodi ponosna, tto funkcijo sem sam napisal pa sem dobil prav, #yeea

neg_loglik_HURDLE <- function(par, freq, expo){
  lambda <- expo*exp(par[1])
  p <- exp(par[2])/(1+exp(par[2]))
  -sum((freq == 0) * (log(p ))) - 
    sum((freq != 0) * (log((1-p)) + dpois(freq, lambda, log = TRUE)-log((1-dpois(0, lambda, log = FALSE)))))
}
fit_HURDLE <- nlm(neg_loglik_HURDLE, c(1, 1), hessian = TRUE, 
              freq = mtpl$nclaims, expo = mtpl$expo)
fit_HURDLE
```

Corresponding standard errors:

```{r, echo=F}
fit_HURDLE_se <- sqrt(diag(solve(fit_HURDLE$hessian)))
fit_HURDLE_se
```

Now that we have calculated AIC for all distributions we can compare the values. The best AIC value is the lowest one.


```{r, echo=F}
AIC <- round(c("AIC POI"= AIC_poi, "AIC NB"=AIC_nb,"AIC ZIP"= AIC_zip, "AIC HURDLE"=AIC_Hurdle))
AIC

```
We can observe that the lowest AIC value is achieved in the Negative Binomial distribution, followed by Zero Inflated Poisson. We can also see that Hurdle distribution has by far the worst AIC. 
**Kaj to pomeni - ne vem

```{r, echo=F}
AIC[which.min(AIC)]
```


```{r, echo=F}

```


```{r, echo=F}

```
