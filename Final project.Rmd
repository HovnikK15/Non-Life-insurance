---
title: "Final project"
output: html_document
author: "Klemen Hovnik and Manca Strgar"
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages,  include=FALSE, warning = FALSE, message = FALSE}
#package instalation:
packages <- c("tidyverse", "here", "gridExtra", "grid", "rstudioapi", "MASS", "actuar", "statmod", "ReIns", "pscl", "rmarkdown", "ggplot2", "graphics", "survival")
new_packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)

if(sum(!(packages %in% installed.packages()[, "Package"]))) {
  stop(paste('The following required packages are not installed:\n', 
             paste(packages[which(!(packages %in% installed.packages()[, "Package"]))], collapse = ', ')));
} else {
  message("Everything is set up correctly. You are ready to go.")
}
#libraries:
library(dplyr)
library(knitr)
library(MASS)
library(rmarkdown)
library(tidyverse)
library(ggplot2)
library(pscl) 
library(actuar)
library(graphics)
library(survival)
```
# 1 Importing dataset

First, we need to import Insurance contract data from the file `NonFleetCo507` into R. The file contains 159.947 contracts with 11 variables. The column `Clm Count` shows how many claims were filed on the contract. This is the loss frequency data that we will use for model fitting. The column `TLength` shows the period of exposure during which the contract was active and the Clm Count was filed. 
Let's have a look at our dataset:

```{r, echo=F}
mtpl_orig <- read.delim("NonFleetCo507.txt", header = TRUE)
mtpl_orig=as_tibble(mtpl_orig)
kable(head(mtpl_orig))
str(mtpl_orig)
#summary(mtpl_orig)
```

We could rename colums in our dataset to make it easier to work with. We will rename the column `Clm_Count` into `nclaims` as the number of claims, and `TLength` into `expo` as exposure. For easier programming we will also rename `AgeInsued` and `SexInsured` into `age` and `sex`. We will also delete colums which are not important for our analysis rightnow. 
```{r, echo=F}
#We don't need all the colums, so we can make new table which has only imporant colums about #insurance contract
mtpl <- mtpl_orig%>% dplyr::select(-PrivateCar, -VehCapCubic, -VehCapTonn)
#dim(mtpl)
mtpl <- mtpl %>%
  # rename all columns 
  rename_all(function(.name) {
    .name %>% 
      # replace all names with the lowercase versions
      tolower 
    })
mtpl <- rename(mtpl, nclaims = clm_count,expo = tlength, sex = sexinsured, age = ageinsured )
kable(head(mtpl))
```

# 2 Empirical analysis

To have a better understanding of our data we can make a graph with number of claims per insurance contract. We can observe that in the majority of cases there is zero claims per contract.

```{r, echo=F}
graph1<- ggplot(mtpl, aes(nclaims)) + geom_bar(col = "red", 
              fill = "red", alpha = 0.5) + 
     labs(y = "Abs freq (in exposure)") +
     ggtitle("MTPL - number of claims per contract")
graph1
```

Looking at this in relative terms, we can see that more than 90% of insurance contract did not have any claims. 

```{r, echo=F}
graph2 <- ggplot(mtpl, aes(nclaims)) + theme_bw()
graph2 + geom_bar(aes(y = (..count..)/sum(..count..)), 
    col = "red", fill = "red", alpha = 0.5) + 
  labs(y = "Relative frequency") +
  ggtitle("MTPL - relative number of claims")
```



Continuing our analysis, we can compute the mean and variance of the number of observed claims. 

```{r, echo = F, warning=FALSE}
mean1 <-  sum(mtpl$nclaims) /length(mtpl$nclaims)
var1 <-  sum((mtpl$nclaims - mean1)^2) /length(mtpl$nclaims)
c(mean = mean1, variance = var1)

```

Because we have different exposures between policyholders, we could take exposure information `expo` into account as well. Then we can calculate empirical claim frequency, per unit of exposure and variance. 

```{r first-risk-calculations-mtpl-2, echo = F, warning=FALSE}
mean2 <- sum(mtpl$nclaims) / sum(mtpl$expo); 
var2 <- sum((mtpl$nclaims-mean2*mtpl$expo)^2)/sum(mtpl$expo) 
#mean2
c(mean = mean2,   variance = var2)
#mtpl %>% summarize(emp_freq = mean2, variance = var2 )
```

If we do the same for each gender we see that claim frequency is higher for males than females.

```{r, echo = F, warning=FALSE, message=FALSE}
mtpl %>%  group_by(sex) %>% summarize(emp_freq = sum(nclaims) / sum(expo))
```

# 3 Fitting different distributions to the loss frequency data

We will fit different distribution to the observed claim count data. We will take exposure into account as well and will fit the distribution using Maximum Likelihood Estimation (MLE).

## 3.1 Poisson distribution

### 3.1.1 Numerical calculation

We start by fitting the Poisson distribution to the observed loss frequency data.


Since, not all policyholders are insured throughout the whole year (exposure = period of exposure during which the contract was active, is not equal to 1 for all the contracts), we assume that the claim intensity is proportional to the exposure. For Poisson distribution this means that the intensity is equal to $\lambda * expo$. For `expo=1` (contract active for a whole year), the expected number of claims equals to $\lambda.$ For all other contracts, the expected number of claims equals  $\lambda * expo$. We denote this with:
$$N_i \sim Poiss(\lambda * expo)$$

Let denote that $m$ is the number of observations and $n_{i}$ is the observed number of claims for i-th policyholder, then general definition of likelihood is given by:
$$L(\lambda)= \prod_{i = 1}^{m} P(N_i = n_i) =\prod_{i = 1}^{m} exp(-\lambda \cdot expo) \cdot \frac{(\lambda \cdot expo)^{n_i}}{n_i!} $$

$$l(\lambda)=\sum_{i = 1}^{m} -\lambda \cdot expo + n_i \cdot log(\lambda \cdot expo) - log(n_i!)$$


We can maximize loglikelihood with respect to $\lambda$. 
```{r, echo=F}
expo = mtpl$expo
nclaims = mtpl$nclaims
```

```{r, echo = F}
poisson.loglikelihood <- function(lambda)
{
  loglikelihood <- sum(-lambda * expo + nclaims * log(lambda * expo) - lfactorial(nclaims))
  
  return(loglikelihood)
}

```
In practice, we minimize the negative log-likelihood so that we can use `nlm` (non-linear minimizer) function for finding the minimum.Minimizing the negative log-likelihood is equal to maximizing the log-likelihood.

$$l(\lambda)=\sum_{i = 1}^{m} \lambda \cdot expo - n_i \cdot log(\lambda \cdot expo) + log(n_i!)$$

Since the parameter $\lambda$ is strickly positive, we will reparametrize the likelihood and optimize for $\beta = log(\lambda)$ which can take values $(-\infty, \infty)$

```{r, echo = F}

poisson.negLoglikelihood <- function(beta)
{
  lambda = exp(beta)
  
  return(-poisson.loglikelihood(lambda))
}
```

Now we will use non-linear minimization function to get the minimum value. The starting value will be set to $\beta = 1$. The function returns the following lines:

- minimum: the value of estimated minimum of our function

- estimate: the point at which that minimum is obtained.

- gradient: first derivative at the estimated minimum 

- hessian: second derivative at the estimated minimum

```{r, echo = F, warning=FALSE}

fit <- nlm(poisson.negLoglikelihood, 1, hessian = TRUE)
fit
```

From this we can get the estimation of $\lambda$ which is identical to the expected value (empirical claim frequency per unit) that we have calculated before.

```{r, echo = F}
poisson.lambda <- exp(fit$estimate) 
poisson.lambda  #estimated value of lambda
```

In general, the covariance matrix of the maximum likelihood estimators can be estimated by the inverse of this hessian of minus the log-likelihood at the estimated minimum. By taking the square root of these diagonal elements, we obtain the corresponding standard error that equals:
```{r, echo = F}
se_nlm_poiss <- sqrt(diag(solve(fit$hessian)))
se_nlm_poiss
```

### 3.1.2 Akaike Information Criterion

Calculation of Akaike Information Criterion (AIC):
Let $k$ be the number of estimated parameters in the model and let $L$ be the maximum value of the likelihood function for the model. Then the AIC value of the model is:
$$AIC = 2k - 2ln(L)$$
For Poisson distribution AIC equals to:
```{r, echo = F}
poisson.minimum <- poisson.loglikelihood(poisson.lambda)
AIC_poi = -2 * poisson.minimum + 2 * 1
AIC_poi
```

### 3.1.3 Calculation with the help of Generalized Linear Models (GLMs)

We will now vertify our solution in the previous section for numerically computed log-likelihood with the `glm` function. We will put focus on the Poisson regression model. Under Poisson assumption in this model, the number of claims is distributed as follows:
$$N \sim Poiss(\mu),$$ $$\mu = expo \cdot exp(x^`\beta).$$

For `glm` function, we will take exposure into account as well. From the box below we can see the `glm` function that we used and the return that it gives. We get that the standrad error equals to 0,0079, AIC equals to 101.670, lambda estimate is -1,86. So we can conclude that both methods give the same results.  
```{r, echo=F, warning=FALSE}
fm_pois <- glm(nclaims ~ 1, offset = log(expo), 
                  family = poisson(link = "log"), 
                  data = mtpl)
summary(fm_pois)
#fm_pois %>% broom::tidy()


```

```{r, echo=F}
se_glm_pois <- summary(fm_pois)$coefficients[,2]
#se_glm_pois
#AIC(fm_pois)
```

## 3.2 Negative Binomial distirbution
### 3.2.1 Numerical calculation

The probability function for the negative binomal distribution is as follows:
$$P(N=k)=\frac{\Gamma(a+k)}{\Gamma(a)k!}(\frac{\mu}{\mu+a})^k(\frac{a}{\mu+a})^a$$
where $\mu$ is the expected number of claims for a policy holder who is insured for a full year. So for each policy holder we can define $\mu_i = expo\cdot \mu.$

The likelihood and loglikelihood function is:
$$L(\lambda)= \prod_{i = 1}^{m} P(N_i = n_i)$$
$$l(\lambda)=\sum_{i = 1}^{m} log(\Gamma(a+k)) - log(\Gamma(a)k!)+k\cdot log(\frac{\mu}{\mu+a})+a\cdot log(\frac{a}{\mu+a}).$$

```{r, echo=F}
#From empirical analysis we know that: 
mu <- sum(mtpl$nclaims) / sum(mtpl$expo)
var <- sum((mtpl$nclaims-mu*mtpl$expo)^2)/sum(mtpl$expo) 

NB.negativeLoglikelihood <- function(beta)
{
  mu <- exp(beta[1])
  a <- exp(beta[2])
  
  loglikelihood <- sum(lgamma(a + nclaims) - lgamma(a) - lfactorial(nclaims) + nclaims * log(mu*expo/(mu*expo + a)) + a * log(a / (mu * expo + a)))
  
  return(-loglikelihood)
}
```

We know that $\mu=E(X)$ and $a=\frac{\mu^2}{Var(X)-\mu}$.

With the `nlm` function we get the following:

```{r, echo=F}
#Starting point:
mean <- mu
a0 <- mu^2 / (var - mu) 
# we know that beta[1] = log(mu) and beta[2]=log(a0)

fit2 <- nlm(NB.negativeLoglikelihood, log(c(mean, a0)),hessian=TRUE)
fit2

```
and we can calculate the corresponding standard error for $\mu$ and $a$ which is equal to:
```{r, echo = F}
se_nlm_nb <- sqrt(diag(solve(fit2$hessian)))
se_nlm_nb
```

* Akaike Information Criterion

For Negative Binomial distribution AIC equals to:

```{r, echo=F}
nb.loglik <- -fit2$minimum
AIC_nb = - 2 * nb.loglik + 2 * 2
AIC_nb
```
### 3.2.2 Calculation with the help of Generalized Linear Models (GLMs)

In the box below we can see what `glm` function returns for Negative Binomial distribution:
```{r, echo=F}
fm_nb <- glm.nb(nclaims ~ 1 + offset(log(expo)), link=log)
summary(fm_nb)
```

```{r, echo=F}
se_glm_nb <- summary(fm_nb)$coefficients[,2]
#se_glm_nb
#AIC(fm_nb)
```
We can see that we get the same values of the estimate, and minimum. 

We can also calculate correspoding standard deviation, which equals to `r se_glm_nb`. Also the value of AIC stays the same. From this we can conclude, that both methods return the same results.

## 3.3 Zero Inflated Poisson  distirbution
### 3.3.1 Numerical calculation
This is a Poisson distribution where the probability of having zero claims is increased by $p$.

$$ P(N^{ZI}=k) =   \left\{
\begin{array}{ll}
      p+(1-p) \cdot P(N=0) ; k=0, \\
      (1-p) \cdot P(N=k) ; k>0 \\
\end{array} 
\right.  $$
where $N$ represents Poisson distribution. 
The parameter $p$ takes valued in $[0,1]$, so we can transform in to the real line $(-\infty,\infty)$ with logarithm:
$$logit(p)=log(\frac{p}{1-p})=\beta$$ 
$$p=\frac{exp(\beta)}{1+exp(\beta)}$$
```{r, echo = F}
ZIP.negativeLoglikelihood <- function(beta)
{
  lambda <- exp(beta[1])
  p <- exp(beta[2])/(1+exp(beta[2]))
  
  density <- (p + (1-p) * exp(-expo * lambda))^(nclaims == 0) * ((1-p) * exp(-expo * lambda) * (expo *lambda)^nclaims / gamma(nclaims+1))^(nclaims != 0) 
  
  loglikelihood <- sum(log(density))
  
  return(-loglikelihood)
}
```

```{r, echo=F, warning=FALSE}
fit3 <- nlm(ZIP.negativeLoglikelihood, c(0, 0),hessian=TRUE)
fit3
```

With `nlm` function we can compute the estimated values for $\lambda$ and $p$.

```{r, echo=F}
ZIP.lambda <- exp(fit3$estimate[1])
ZIP.p <- exp(fit3$estimate[2])/(1+exp(fit3$estimate[2]))
c(lambda = ZIP.lambda, p = ZIP.p)

```

The corresponding standard error under Zero Inflated distribution for both parameters equals to:
```{r, echo = F}
se_nlm_zip <- sqrt(diag(solve(fit3$hessian)))
se_nlm_zip
```

* Akaike Information Criterion

For Zero Inflated Poisson distribution AIC equals to:
```{r, echo=F}
ZIP.loglik <- -fit3$minimum
AIC_zip = -2 * ZIP.loglik + 2 * 2
AIC_zip
```

### 3.3.2 Calculation with the help of Generalized Linear Models (GLMs)

```{r, echo=F}
fm_zip <- zeroinfl(nclaims ~ 1, offset = log(expo), 
                  dist = "poisson")
summary(fm_zip)


```


```{r, echo=F}
se_glm_zip <- c((summary(fm_zip)$coefficients)$count[2], (summary(fm_zip)$coefficients[2])$zero[2])
#se_glm_zip
#AIC(fm_zip)
```
We can see that the `glm` function returns the same values of the estimate, and minimum. 

We can also calculate correspoding standard deviation, which equals to `r se_glm_zip`. The value of AIC also stays the same.

so we can conclude, that both methods return the same results.

## 3.4 Hurdle Poisson distirbution
### 3.4.1 Numerical calculation
In Hurlde Poisson the probability of observing zero claims is set to $p$. The probability of observing $k$ claims equals:
$$ P(N^{H}=k) =   \left\{
\begin{array}{ll}
      p ; k=0, \\
      (1-p) \cdot \frac{(N=k)}{1-P(N=0)} ; k>0 \\
\end{array} 
\right.  $$
Here the probability $p$ does not depend on the exposure, while intensity $\lambda$ is proportional to exposure as: $\lambda_i=expo\cdot \lambda.$

```{r, echo=F}
Hurdle.negativeLoglikelihood <- function(beta)
{
  lambda <- exp(beta[1])
  p <- exp(beta[2])/(1+exp(beta[2]))
  
  density <- (p)^(nclaims == 0) * ((1-p) * exp(-expo * lambda) / (1-exp(-lambda * expo)) * (expo *lambda)^nclaims / gamma(nclaims+1))^(nclaims != 0) 
  
  loglikelihood <- sum(log(density))
  
  return(-loglikelihood)
}
```


```{r, echo=F, warning=FALSE}
fit4 <- nlm(Hurdle.negativeLoglikelihood, c(0, 0),hessian=TRUE)
fit4
```

With `nlm` function we can compute the estimated values for $\lambda$ and $p$.

```{r, echo=F}
Hurdle.lambda <- exp(fit4$estimate[1])
Hurdle.p <- exp(fit4$estimate[2])/(1+exp(fit4$estimate[2]))
c(lambda = Hurdle.lambda, p = Hurdle.p)

```

The corresponding standard error for both parameters equals:
```{r, echo = F}
se_nlm_hurdle <- sqrt(diag(solve(fit4$hessian)))
se_nlm_hurdle
```

* Akaike Information Criterion

For Hurdle Poisson distribution AIC equals to:

```{r, echo=F}
Hurdle.loglik <- -fit4$minimum
AIC_Hurdle = -2 * Hurdle.loglik + 2 * 2
AIC_Hurdle
```

### 3.4.2 Calculation with the help of Generalized Linear Models (GLMs)

```{r, echo=F}
fm_hurdle <- hurdle(nclaims ~ 1, offset=(log(expo)), dist= "poisson",
zero.dist=c("binomial"))
summary(fm_hurdle)

```

```{r, echo=F}
se_glm_hurdle <- c(summary(fm_hurdle)$coefficients$count[2], (summary(fm_hurdle)$coefficients[2])$zero[2])
#se_glm_hurdle
#AIC(fm_hurdle)
```
We can see that we get the same values of the estimate, and minimum. 

We can also calculate correspoding standard deviation, which equals to `r se_glm_hurdle`. The value of AIC also stays the same.

We can conclude, that both methods return the same results.

---------------------------------------------------------------------------------------------
Now that we have calculated AIC for all distributions we can compare the values. The best AIC value is the lowest one.


```{r, echo=F}
AIC <- round(c("AIC POI"= AIC_poi, "AIC NB"=AIC_nb,"AIC ZIP"= AIC_zip, "AIC HURDLE"=AIC_Hurdle))
AIC

```
We can observe that the lowest AIC value is achieved with the Negative Binomial distribution and equals to 101.318, followed by Zero Inflated Poisson. We can also see that Hurdle distribution has by far the worst AIC. 

```{r, echo=F}
#AIC[which.min(AIC)]
```

# 4 Comaring frequency models

We will now compare the frequency models by comparing the expected number of zeros with the actually observed number of zero claims. 
First we will calculate the actual number of zero claims per contracts in our dataset.

```{r, echo =F}
#actual number of zero claims in our data:
actual_zero <- sum(nclaims < 1)
#actual_zero
```
We compute that there is `r actual_zero` insurance contracts that did not have any claims, out of `r length(nclaims)` contracts.

* Poisson distribution


The predicted number of zero claims per contract under Poisson distribution is equal to:
```{r, echo=F}
#First we apply fitted and combine with dpois --> number of expected zeros:
fit_pois_zero <- round(sum(dpois(0, fitted(fm_pois))))
fit_pois_zero
difference <- actual_zero - fit_pois_zero
#difference

diff_percentage <- round(difference/actual_zero *100,2)
#diff_percentage
```
This means that our model returns pretty accurate number of expected zeros. The difference is `r difference` which represents only `r diff_percentage` % deviation.

* Negative Binomial distribution
The predicted number of zero claims per contract under Negative Binomial distribution is equal to:
```{r, echo=F}
theta <- fm_nb$theta  #dispersion parameter
fit_nb_zero <- round(sum(dnbinom(0, mu=fitted(fm_nb), size=theta)))
fit_nb_zero
difference <- abs(actual_zero - fit_nb_zero)
#difference
diff_percentage <- round(difference/actual_zero *100,3)
#diff_percentage
```
This means that our model returns very accurate number of expected zeros. The difference is only `r difference` zeros.

* Zero Inflated Poisson distribution

The predicted number of zero claims per contract under ZIP is equal to:
```{r, echo=F}

fit_zip_zero <- round(sum(predict(fm_zip, type="prob")[,1]))
fit_zip_zero
difference <- abs(actual_zero - fit_zip_zero)
#difference
diff_percentage <- round(difference/actual_zero *100,3)
#diff_percentage
```
This means that our model returns very accurate number of expected zeros. The difference is only `r difference` zeros.

* Hurdle distribution

The predicted number of zero claims per contract under Hurdle distribution is equal to:
```{r, echo=F}

fit_hurdle_zero <- round(sum(predict(fm_hurdle, type="prob")[,1]))
fit_zip_zero
difference <- abs(actual_zero - fit_hurdle_zero)
#difference
diff_percentage <- round(difference/actual_zero *100,3)
#diff_percentage
```
Which means that our model gives us the same number of expected zeros as there are actual zeros in our dataset.

From accuracy prespective the Hurdle distribution estimates the number of zeros the most accurately, while Poisson distribution returns the biggest (yet very small) deviation.

